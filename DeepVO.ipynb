{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-07ce62dec2ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmpimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.gridspec as gridspec\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create dataframe with image_path, time(seconds) and speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* In the video given, we have ~344 (5min 44s) seconds of video. \n",
    "* Our ground truth labels correspond to a video that is 12m 12s (~732seconds). \n",
    "* We only have a portion of that video. \n",
    "* It appears that our framerate <strong>~ 13 fps </strong>. (4459 frames * (1 second / 13frames) = 344 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-ae9a625e109e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/driving.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./data/driving.csv')\n",
    "df.head(10)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c726853e27bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvalid_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3541\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4459\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3540\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "valid_data = df[3541:4459]\n",
    "train_data = df[0:3540]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-9fb39b8e230a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtimes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mspeeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'speed'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r-'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Speed vs Time'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "times = np.asarray(df['time'], dtype = np.float32)\n",
    "speeds = np.asarray(df['speed'], dtype=np.float32)\n",
    "plt.plot(times, speeds, 'r-')\n",
    "plt.title('Speed vs Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-fcb21af36c2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtail\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def change_brightness(image):\n",
    "    \"\"\"\n",
    "    Augments the brightness of the image by multiplying the saturation by a uniform random variable\n",
    "    Input: image (RGB)\n",
    "    returns: image with brightness augmentation\n",
    "    \"\"\"\n",
    "    bright_factor = 0.2 + np.random.uniform()\n",
    "    \n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "    # perform brightness augmentation only on the second channel\n",
    "    hsv_image[:,:,2] = hsv_image[:,:,2] * bright_factor\n",
    "    \n",
    "    # change back to RGB\n",
    "    image_rgb = cv2.cvtColor(hsv_image, cv2.COLOR_HSV2RGB)\n",
    "    return image_rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perspective Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_perspective_transform(image):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    \"\"\"\n",
    "    preprocesses the image\n",
    "    \n",
    "    input: image (480 (y), 640 (x), 3) RGB\n",
    "    output: image (shape is (256, 256, 3) as RGB)\n",
    "    \n",
    "    This stuff is performed on my validation data and my training data\n",
    "    Process: \n",
    "             1) Cropping out black spots\n",
    "             2) Perspective transform\n",
    "             3) resize to (256, 256, 3) if not done so already from perspective transform\n",
    "    \"\"\"\n",
    "    # Crop out sky (top) (100px) and black right part (-90px)\n",
    "    image_cropped = image[100:440, :-90] # -> (380, 550, 3)\n",
    "    \n",
    "    # TODO: Write perspective transform \n",
    "#     perspective = apply_perspective_transform(image_cropped)\n",
    "    \n",
    "    image = cv2.resize(image_cropped, (227, 227), interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_image_valid_from_path(image_path, speed):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = preprocess_image(img)\n",
    "    return img, speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_image_from_path(image_path, speed):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    img = change_brightness(img)\n",
    "\n",
    "            \n",
    "    img = preprocess_image(img)\n",
    "    return img, speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_training_data(data, batch_size = 32):\n",
    "    image_batch = np.zeros((batch_size, 227, 227, 3)) # nvidia input params\n",
    "    label_batch = np.zeros((batch_size))\n",
    "    while True:\n",
    "        for i in range(batch_size):\n",
    "            idx = np.random.randint(len(data) - 1)\n",
    "            row1 = data.iloc[[idx]].reset_index()\n",
    "            row2 = data.iloc[[idx + 1]].reset_index()\n",
    "            x1, y1 = preprocess_image_from_path(row1['image_path'].values[0], row1['speed'].values[0])\n",
    "            \n",
    "            # preprocess another image\n",
    "            x2, y2 = preprocess_image_from_path(row2['image_path'].values[0], row2['speed'].values[0])\n",
    "           \n",
    "            # stack images\n",
    "            x = np.concatenate(x1, x2, axis = 0)\n",
    "            \n",
    "            # calculate mean speed\n",
    "            y = np.mean([y1, y2])\n",
    "            \n",
    "            image_batch[i] = x\n",
    "            label_batch[i] = y\n",
    "            \n",
    "        yield shuffle(image_batch, label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stack mode\n",
    "def generate_validation_data(data):\n",
    "    while True:\n",
    "        for idx in range(len(data) - 1):\n",
    "            row1 = data.iloc[[idx]].reset_index()\n",
    "            row2 = data.iloc[[idx + 1]].reset_index()\n",
    "            \n",
    "            x1, y1 = preprocess_image_valid_from_path(row1['image_path'].values[0], row1['speed'].values[0])\n",
    "            x2, y2 = preprocess_image_valid_from_path(row2['image_path'].values[0], row2['speed'].values[0])\n",
    "            \n",
    "            s1 = row1['speed'].values[0]\n",
    "            s2 = row2['speed'].values[0]\n",
    "            \n",
    "            # Stack the images to send into the model\n",
    "            img_stack = np.concatenate(x1, x2, axis = 0)\n",
    "            \n",
    "            img_stack = img_stack.reshape(1, img_stack.shape[0], img_stack.shape[1], img_stack.shape[2])\n",
    "            \n",
    "            y = np.mean([y1, y2])\n",
    "            \n",
    "            speed = np.array([[y]])\n",
    "            yield img_stack, speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Architecture (DeepVO AlexNetLike)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Channel Normalization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-ebabec72e5f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m  \u001b[0mLambda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMerge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvolutional\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConvolution2D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers.core import  Lambda, Merge\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.engine import Layer\n",
    "\n",
    "def crosschannelnormalization(alpha = 1e-4, k=2, beta=0.75, n=5,**kwargs):\n",
    "    \"\"\"\n",
    "    This is the function used for cross channel normalization in the original\n",
    "    Alexnet\n",
    "    \"\"\"\n",
    "    def f(X):\n",
    "        b, ch, r, c = X.shape\n",
    "        half = n // 2\n",
    "        square = K.square(X)\n",
    "        extra_channels = K.spatial_2d_padding(K.permute_dimensions(square, (0,2,3,1))\n",
    "                                              , (0,half))\n",
    "        extra_channels = K.permute_dimensions(extra_channels, (0,3,1,2))\n",
    "        scale = k\n",
    "        for i in range(n):\n",
    "            scale += alpha * extra_channels[:,i:i+ch,:,:]\n",
    "        scale = scale ** beta\n",
    "        return X / scale\n",
    "\n",
    "    return Lambda(f, output_shape=lambda input_shape:input_shape,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-ce863e427904>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmerge\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpooling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvolutional\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConvolution2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mZeroPadding2D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Activation, Flatten, merge, Lambda, Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.convolutional import Convolution2D, ZeroPadding2D\n",
    "from keras.optimizers import Adam\n",
    "from CustomLayers import LRN2D\n",
    "\n",
    "\n",
    "tf.python.control_flow_ops = tf\n",
    "\n",
    "\n",
    "N_img_height = 227\n",
    "N_img_width = 227\n",
    "N_img_channels = 3\n",
    "\n",
    "# inputShape = (N_img_height, N_img_width, N_img_channels)\n",
    "def splitTensor(tensor):\n",
    "    \"\"\"\n",
    "    Out inputs come in as two stacked (?, 227, 227 ,3) images \n",
    "    tensor shape: (?, 454, 227, 3)\n",
    "    output: split1: tensor (?, 227, 227, 3)\n",
    "            split2: tensor (?, 227, 227, 3)\n",
    "    \"\"\"\n",
    "    split1 = tensor[:, 0:227,:,:]\n",
    "    split2 = tensor[:, 227:454,:,:]\n",
    "    return split1, split2\n",
    "    \n",
    "\n",
    "def DeepVo():\n",
    "    # TODO: Maybe add zeroPadding2D(1, 1) after each layer\n",
    "    # TODO: Maybe take out input_shape = inputShape from first conv layer\n",
    "    \n",
    "    ALPHA = 0.00001\n",
    "    BETA = 0.75\n",
    "    DIM_ORDERING = 'tf'\n",
    "    \n",
    "    \n",
    "    inputShape = (N_img_height * 2, N_img_width, N_img_channels)\n",
    "    inputShapeSingle = (N_img_height, N_img_width, N_img_channels)\n",
    "    \n",
    "    inputImg = Input(shape = inputShape)\n",
    "    \n",
    "#     x = Lambda(lambda x: x / 127.5 - 1, input_shape = inputShape)\n",
    "    \n",
    "#     print('inputImg shape:: ', inputImg.get_shape)\n",
    "    \n",
    "    x, y = splitTensor(inputImg)\n",
    "    \n",
    "    x = Lambda(lambda a: a / 127.5 - 1, input_shape = inputShapeSingle)(x)\n",
    "    \n",
    "    y = Lambda(lambda b: b / 127.5 - 1, input_shape = inputShapeSingle)(y)\n",
    "    \n",
    "#     print('x shape: ', x.get_shape)\n",
    "#     print('y shape: ', y.get_shape)\n",
    "    print(type(x))\n",
    "    \n",
    "    ######## LAYER 1 ####################\n",
    "    # Convolution Layer 1: Channel 1\n",
    "    x1 = Convolution2D(96, 11, 11, subsample=(4, 4), \n",
    "                      init = 'he_normal', \n",
    "                      border_mode = 'valid', \n",
    "                      activation = 'relu',\n",
    "                      input_shape = inputShapeSingle,  # may not be necessary\n",
    "                      name = 'conv_1_x')(x)\n",
    "\n",
    "    # Convolution Layer 1: Channel 2\n",
    "    y1 = Convolution2D(96, 11, 11, subsample=(4, 4), \n",
    "                      init = 'he_normal', \n",
    "                      border_mode = 'valid', \n",
    "                      activation = 'relu',\n",
    "                      input_shape = inputShapeSingle,  # may not be necessary\n",
    "                      name = 'conv_1_y')(y)\n",
    "    ### TODO: ADD NORMALIZATION to X1\n",
    "#     x1 = crosschannelnormalization(x1)\n",
    "    \n",
    "    \n",
    "    ### TODO: ADD NORMALIZATION TO Y1\n",
    "    \n",
    "    # Max Pooling Layer 1: Channel 1\n",
    "    x1 = MaxPooling2D((3, 3), strides = (2, 2), dim_ordering=DIM_ORDERING)(x1)\n",
    "    # Max Pooling Layer 1: Channel 2\n",
    "    y1 = MaxPooling2D((3, 3), strides = (2, 2), dim_ordering=DIM_ORDERING)(y1)\n",
    "    ######## END OF LAYER 1 ####################\n",
    "\n",
    "    ######## LAYER 2####################\n",
    "    # Convolution Layer 2: Channel 1\n",
    "    x1 = Convolution2D(256, 5, 5, subsample = (1, 1), \n",
    "                      init = 'he_normal', \n",
    "                      border_mode = 'valid', \n",
    "                      activation = 'relu', \n",
    "                      name = 'conv_2_x')(x1)\n",
    "\n",
    "    # Convolution Layer 2: Channel 2\n",
    "    y1 = Convolution2D(256, 5, 5, subsample = (1, 1), \n",
    "                      init = 'he_normal', \n",
    "                      border_mode = 'valid', \n",
    "                      activation = 'relu', \n",
    "                      name = 'conv_2_y')(y1)\n",
    "    \n",
    "    # Zero Padding (2, 2) on ConvLayer2: Channel 1\n",
    "    x1 = ZeroPadding2D(padding = (2, 2), dim_ordering=DIM_ORDERING)(x1)\n",
    "\n",
    "    # Zero Padding (2, 2) on ConvLayer2: Channel 2\n",
    "    y1 = ZeroPadding2D(padding = (2, 2), dim_ordering=DIM_ORDERING)(y1)\n",
    "    \n",
    "    #TODO: Add cross channel normalizations\n",
    "    \n",
    "    # Max Pooling Layer 1: Channel 1\n",
    "    x1 = MaxPooling2D((3, 3), strides = (2, 2))(x1)\n",
    "    \n",
    "    # Max Pooling Layer 1: Channel 2\n",
    "    y1 = MaxPooling2D((3, 3), strides = (2, 2))(y1)\n",
    "    \n",
    "    ####### LAYER 3 ###################\n",
    "    # Convolution Layer 3: Channel 1\n",
    "    x1 = Convolution2D(384, 3, 3, subsample = (1, 1), \n",
    "                  init = 'he_normal', \n",
    "                  border_mode = 'valid', \n",
    "                  activation = 'relu', \n",
    "                  name = 'conv_3_x')(x1)\n",
    "\n",
    "    # Convolution Layer 3: Channel 2\n",
    "    y1 = Convolution2D(384, 3, 3, subsample = (1, 1), \n",
    "                  init = 'he_normal', \n",
    "                  border_mode = 'valid', \n",
    "                  activation = 'relu', \n",
    "                  name = 'conv_3_y')(y1)\n",
    "    \n",
    "    # Zero Padding (1, 1) on Layer 3 channel 1\n",
    "    x1 = ZeroPadding2D(padding = (1, 1), dim_ordering=DIM_ORDERING)(x1)\n",
    "    \n",
    "    # Zero Padding (1, 1) on Layer 3 channel 2\n",
    "    y1 = ZeroPadding2D(padding = (1, 1), dim_ordering=DIM_ORDERING)(y1)\n",
    "    \n",
    "    \n",
    "    ####### LAYER 4 ###################\n",
    "    # Convolution Layer 4: Channel 1\n",
    "    x1 = Convolution2D(384, 3, 3, subsample = (1, 1), \n",
    "                  init = 'he_normal', \n",
    "                  border_mode = 'valid', \n",
    "                  activation = 'relu', \n",
    "                  name = 'conv_4_x')(x1)\n",
    "\n",
    "    # Convolution Layer 4: Channel 2\n",
    "    y1 = Convolution2D(384, 3, 3, subsample = (1, 1), \n",
    "                  init = 'he_normal', \n",
    "                  border_mode = 'valid', \n",
    "                  activation = 'relu', \n",
    "                  name = 'conv_4_y')(y1)\n",
    "    \n",
    "    # Zero Padding (1, 1) on Layer 4 channel 1\n",
    "    x1 = ZeroPadding2D(padding = (1, 1), dim_ordering=DIM_ORDERING)(x1)\n",
    "    \n",
    "    # Zero Padding (1, 1) on Layer 4 channel 2\n",
    "    y1 = ZeroPadding2D(padding = (1, 1), dim_ordering=DIM_ORDERING)(y1)\n",
    "    \n",
    "    ####### LAYER 5 ###################\n",
    "    # Convolution Layer 5: Channel 1\n",
    "    x1 = Convolution2D(256, 3, 3, subsample = (1, 1), \n",
    "                  init = 'he_normal', \n",
    "                  border_mode = 'valid', \n",
    "                  activation = 'relu', \n",
    "                  name = 'conv_5_x')(x1)\n",
    "\n",
    "    # Convolution Layer 5: Channel 2\n",
    "    y1 = Convolution2D(256, 3, 3, subsample = (1, 1), \n",
    "                  init = 'he_normal', \n",
    "                  border_mode = 'valid', \n",
    "                  activation = 'relu', \n",
    "                  name = 'conv_5_y')(y1)\n",
    "    \n",
    "    \n",
    "    # Zero Padding (1, 1) on Layer 5 channel 1\n",
    "    x1 = ZeroPadding2D(padding = (1, 1), dim_ordering=DIM_ORDERING)(x1)\n",
    "    \n",
    "    # Zero Padding (1, 1) on Layer 5 channel 2\n",
    "    y1 = ZeroPadding2D(padding = (1, 1), dim_ordering=DIM_ORDERING)(y1)\n",
    "    \n",
    "    \n",
    "    # Pool Layer 5\n",
    "    # Max Pooling Layer 5: Channel 1\n",
    "    x1 = MaxPooling2D((3, 3), strides = (2, 2), dim_ordering=DIM_ORDERING )(x1)\n",
    "    \n",
    "    # Max Pooling Layer 5: Channel 2\n",
    "    y1 = MaxPooling2D((3, 3), strides = (2, 2), dim_ordering=DIM_ORDERING)(y1)\n",
    "    \n",
    "    # reduce from 4 dimensions (?, 6, 6, 256) -> (?, 43264) so its FLAT\n",
    "    x2 = Convolution2D(9216, 6, 6, init = \"he_normal\", border_mode = 'valid', activation = 'relu')(x1)\n",
    "    x2 = tf.reshape(x2, [-1, 9216], name = 'flattened')\n",
    "    \n",
    "    y2 = Convolution2D(9216, 6, 6, init = \"he_normal\", border_mode = 'valid', activation = 'relu')(y1)\n",
    "    y2 = tf.reshape(y2, [-1, 9216], name = 'flattened')\n",
    "    \n",
    "    # TODO: Maybe add a relu after these layers\n",
    "    ###### Fully Connected 1 using Xaviers algorithm (glorot normal) ########\n",
    "    x3 = Dense(4096, activation = 'relu', init = 'glorot_normal', name = 'fc1_x2')(x2)\n",
    "    x3 = Dropout(0.5)(x3)\n",
    "    \n",
    "    y3 = Dense(4096, activation = 'relu', init = 'glorot_normal', name = 'fc1_y2')(y2)\n",
    "    y3 = Dropout(0.5)(y3)\n",
    "    \n",
    "    ###### Fully Connected 2 ########\n",
    "    x3 = Dense(4096, activation = 'relu', init = 'glorot_normal', name = 'fc2_x2')(x3)\n",
    "    x3 = Dropout(0.5)(x3)\n",
    "    \n",
    "    y3 = Dense(4096, activation = 'relu', init = 'glorot_normal', name = 'fc2_y2')(y3)\n",
    "    y3 = Dropout(0.5)(y3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ##### Merge ####### \n",
    "    # TODO: Try concatenation instead of merging\n",
    "    m3 = merge([x3, y3], mode='concat', concat_axis = 1, name = 'merge_x3y3')\n",
    "        \n",
    "    #### Fully connected 3 ######\n",
    "    fc3 = Dense(8192, activation = 'relu', init = 'glorot_normal', name = 'fc3')(m3)\n",
    "    \n",
    "    ### Fully connected 4 ######\n",
    "    fc4 = Dense(1024, activation = 'relu', init = 'glorot_normal', name = 'fc4')(fc3)\n",
    "    \n",
    "    ### Fully connected 5 ######\n",
    "    prediction = Dense(1, name = 'output')(fc4)\n",
    "    \n",
    "    OUTPUT = Lambda(lambda x: x)(prediction)\n",
    "    \n",
    "    print('prediction: ', type(prediction))\n",
    "    print('prediction shape: ', prediction.get_shape)\n",
    "    \n",
    "    model = Model(input = inputImg, output = OUTPUT)\n",
    "    \n",
    "    # maybe make adam optimizer beta = 0.75\n",
    "    adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(optimizer = adam, loss = 'mse')\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually create testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'valid_data' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-a95edc885759>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mval_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mvalid_generator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_validation_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mBATCH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'val_size: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'valid_data' is not defined"
     ]
    }
   ],
   "source": [
    "val_size = len(valid_data.index)\n",
    "valid_generator = generate_validation_data(valid_data)\n",
    "BATCH = 16\n",
    "print('val_size: ', val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'DeepVo' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-31687a883667>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDeepVo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtrain_generator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_training_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBATCH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     history = model.fit_generator(\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DeepVo' is not defined"
     ]
    }
   ],
   "source": [
    "model = DeepVo()\n",
    "train_size = len(train_data.index)\n",
    "for i in range(3):\n",
    "    train_generator = generate_training_data(train_data, BATCH)\n",
    "    history = model.fit_generator(\n",
    "            train_generator, \n",
    "            samples_per_epoch = 20480, # try putting the whole thing in here in the future\n",
    "            nb_epoch = 6,\n",
    "            validation_data = valid_generator,\n",
    "            nb_val_samples = val_size)\n",
    "    print(history)\n",
    "    \n",
    "    model.save_weights('model-weights-F1.h5')\n",
    "    model.save('model-F1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'DeepVo' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-4898d8ab308d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodelx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDeepVo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model created!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DeepVo' is not defined"
     ]
    }
   ],
   "source": [
    "modelx = DeepVo()\n",
    "print('model created!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}